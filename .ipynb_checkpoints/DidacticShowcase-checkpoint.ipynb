{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From-scratch implementation of Latent Dirichlet Allocation\n",
    "In this notebook, we will implement LDA as described in the original paper by Blei (2002). Training will be done via Gibbs Sampling, and the dataset we will use will be the speeches of the 65th session of the United Nations in 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the documents of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 189 documents.\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for document in os.listdir(f\"./UN_Session65_2010\"):\n",
    "    if document[0] == \".\":\n",
    "        continue\n",
    "    text = open(f\"./UN_Session65_2010/{document}\", \"r\").read()\n",
    "    corpus.append(text)\n",
    "\n",
    "    \n",
    "print(f\"Corpus length: {len(corpus)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For didcatic purposes, we will only keep the first 20 documents to speed up the Gibbs Sampling. Feel free to increase the number of values if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the first 300 characters of the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although the global \n",
      "economic situation has improved considerably, it is \n",
      "still fragile. Much of the relief has come from the \n",
      "massive liquidity that has been pumped into the global \n",
      "financial system by national Governments. That bought \n",
      "us time to restructure our economies and correct the \n",
      "underlyi...\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "In this section we will preporcess our documents. Preprocessing is a necessary first step in topic modeling as it reduces word variance.\n",
    "\n",
    "The first thing we will do is to lemmatize the words in each one of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\": corpus})\n",
    "\n",
    "pattern = re.compile(\"\\n*[0-9]+\\.\\t\\s*\")\n",
    "df.text = df.text.apply(lambda x: re.sub(pattern, \"\", x))\n",
    "\n",
    "# lemmatization\n",
    "df[\"lemmas\"] = [[[token.lemma_ if token.lemma_ != \"-PRON-\" else token.text.lower() \n",
    "           for token in sentence if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"X\"}]\n",
    "          for sentence in nlp(speech).sents] for speech in df.text]\n",
    "\n",
    "df.to_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then get rid of unnecessary punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(\"\"\"!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\"\")\n",
    "\n",
    "instances = [[lemma for lemmatized_sentence in lemmatized_speech for lemma in lemmatized_sentence if lemma not in punctuation] \n",
    "                    for lemmatized_speech in df.lemmas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will filter words that are below or above a given occurrency threshold. We do this because it is really hard to assign a topic to a word that appears only once across the entire corpus, jut like it is hard to assign a topic to a word that appears almost everywhere. In this case, we will filter out words that appear less than 3 time across the corpus, and words that appear in more than 70% of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1020 unique tokens: ['able', 'account', 'accountable', 'act', 'add']...)\n"
     ]
    }
   ],
   "source": [
    "# filter words that are below/above a given occurrency\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(instances)\n",
    "dictionary.filter_extremes(no_below = 3, no_above = 0.7)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = list(dictionary.token2id.keys())\n",
    "\n",
    "instances = [[e for e in speech if e in filtered_words] for speech in instances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our first document has changed after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improve fragile much come system government economy lead place happen fast enough open question view system renew process destruction sound practice difficult political leader stand when company go job lose like part destruction mean lose election try avoid production factor real labour government b...\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(instances[0])[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "In this section, we will implement the Gibbs sampling algorithm to continuously sample from the full conditional posterior distribution of the topic assigned to each word in each document. Ideally, the Markov Chain should see convergence as iterations increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating three variables that will be core to the Gibbs sampling procedure:\n",
    "- The vocabulary V, collection of unique words that appear across the corpus\n",
    "- N, a list of numbers where each number represents the length of a document\n",
    "- M, the number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 1020\n",
      "Length of the first 5 speeches: [477, 533, 600, 591, 455]\n",
      "Number of documents in corpus: 20\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary V\n",
    "V = sorted(set([e for speech in instances for e in speech]))\n",
    "print(f\"Vocabulary length: {len(V)}\")\n",
    "\n",
    "# Ns - length of each document\n",
    "N = [len(speech) for speech in instances]\n",
    "print(f\"Length of the first 5 speeches: {N[:5]}\")\n",
    "\n",
    "# number of documents in corpus\n",
    "M = len(N)\n",
    "print(f\"Number of documents in corpus: {M}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the prior hyperparameters of our model. Ideally, these should reflect some kind of prior knowledge we have about our corpus:\n",
    "- K is the number of topic we want to extract from our corpus\n",
    "- alpha is the parameter of the Dirichlet distribution that regulates per-document topic mixture. In simpler words, higher alpha will tell the model that all topics appear in all documents in a homogeneous fashion (1/K). lower values of alpha will tell the model that each document is characterized by few topics, and alpha=1 will not prefer any kind of combination of topics per document.\n",
    "- beta has the same function as alpha, but for the per-topic distribution of the elements of the vocabulary V. Low beta will lead the extracted topics to be characterized by few, relevant words; higher values of beta will produce a more homogeneous distribution of words per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LDA model with 10 topics, ùõº=5.0, ùõΩ=0.01\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS - ARBITRARY\n",
    "\n",
    "# number of topics\n",
    "K = 10\n",
    "topics = list(range(K))\n",
    "\n",
    "# alpha - parameter of the dirichlet on the topic mixture\n",
    "alpha = 50/K\n",
    "\n",
    "# beta - parameter of the dirichlet on the words per topic\n",
    "beta = 0.01\n",
    "\n",
    "print(f\"Initializing LDA model with {K} topics, ùõº={alpha}, ùõΩ={beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our hyperparameters are in place, we start our Gibbs Sampling procedure. Step 0 consists in randomly assigning a latent topic to each word in each document. Note: for example the word \"nation\" could be assigned to topic 5 the first time it appears, and to topic 3 the second time it appears. We do this by creating the list of lists Z, where each sub-list represents a document, and each element in each sublist represents the topic assigned to each word in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned each word in each document to a random topic.\n"
     ]
    }
   ],
   "source": [
    "# random topic assignment to each word in each document\n",
    "Z = [[random.sample(topics, 1)[0] for word in speech] for speech in instances]\n",
    "print(\"Assigned each word in each document to a random topic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the N_d_topic matrix, which displays the occurrence of each topic in each document. In the original paper by Blei, this matrix is called theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created N_{d,topic} matrix. It dispays the occurrence of each topic (col) in each document (row).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>61</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>40</td>\n",
       "      <td>74</td>\n",
       "      <td>55</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>81</td>\n",
       "      <td>56</td>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9\n",
       "0  44  48  56  43  50  53  36  47  48  52\n",
       "1  53  51  45  61  56  47  51  53  60  56\n",
       "2  63  66  40  74  55  60  66  62  63  51\n",
       "3  60  57  81  56  50  64  52  53  65  53\n",
       "4  36  46  50  43  42  50  35  40  47  66"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_d_topic = []\n",
    "for document in Z:\n",
    "    N_d = []\n",
    "    for topic in topics:\n",
    "        N_d.append(sum([1 for e in document if e == topic]))\n",
    "    N_d_topic.append(N_d)\n",
    "print(\"Created N_{d,topic} matrix. It dispays the occurrence of each topic (col) in each document (row).\")    \n",
    "\n",
    "N_d_topic = pd.DataFrame(N_d_topic)\n",
    "N_d_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create the (V X K) N_word_topic matrix, which displays the occurrence of each topic (col) in each word (row). In the original paper, it is called beta and is transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created N_{word,topic} matrix. It displays the occurrence of each topic (col) in each word (row).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>More</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerate</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  1  2  3  4  5  6  7  8  9\n",
       "More        0  1  0  0  1  1  0  0  1  0\n",
       "ability     1  2  2  2  0  1  2  1  1  0\n",
       "able        0  2  4  1  1  4  3  1  2  2\n",
       "about       0  0  0  1  0  0  1  0  0  1\n",
       "accelerate  0  1  0  1  1  0  0  0  1  1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_instances = [e for sublist in instances for e in sublist]\n",
    "flat_Z = [e for sublist in Z for e in sublist]\n",
    "\n",
    "N_word_topic = {}\n",
    "for v in V:\n",
    "    N_word_topic[v] = [0 for e in topics]\n",
    "\n",
    "\n",
    "for word, assigned_topic in zip(flat_instances, flat_Z):\n",
    "    for topic in topics:\n",
    "        if assigned_topic == topic:\n",
    "            N_word_topic[word][topic] += 1\n",
    "print(\"Created N_{word,topic} matrix. It displays the occurrence of each topic (col) in each word (row).\")\n",
    "\n",
    "N_word_topic = pd.DataFrame(N_word_topic).T\n",
    "N_word_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our Gibbs Sampling. At each epoch, we will iterate through each word in each document, sampling a new latent topic for that specific word *conditional on the topic assignments of all the other words across the corpus*.\n",
    "\n",
    "For the mathematical details and derivation, refer to the attached paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA_GibbsSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [19:34<00:00, 58.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import sys\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# GIBBS SAMPLING\n",
    "for iteration in trange(epochs, file=sys.stdout, desc='LDA_GibbsSampling'):\n",
    "    #iterate through each document...\n",
    "    for i_doc in range(len(instances)):\n",
    "        # then through each word in each document...\n",
    "        for i_word in range(len(instances[i_doc])):\n",
    "            word = instances[i_doc][i_word]\n",
    "            topic_assigned = Z[i_doc][i_word]\n",
    "\n",
    "            N_d_topic.iloc[i_doc, topic_assigned] -= 1\n",
    "            N_word_topic.loc[word, topic_assigned] -= 1\n",
    "            \n",
    "            # compute the un-normalized full conditional posterior for the latent topic of our current word...\n",
    "            percs = []\n",
    "            for topic in topics:\n",
    "                perc = (N_d_topic.iloc[i_doc, topic] + alpha) * (N_word_topic.loc[word, topic] + beta) / np.sum(N_word_topic.apply(\"sum\").values + beta)\n",
    "                if perc < 0:\n",
    "                    raise Exception(\"perc < 0\")\n",
    "                percs.append(perc)\n",
    "            \n",
    "            # normalize it...\n",
    "            s = np.sum(percs)\n",
    "            percs = [e/s for e in percs]\n",
    "            # and sample a new latent topic for our current word.\n",
    "            new_topic = np.random.choice(topics, 1, p=percs)[0]\n",
    "\n",
    "            N_d_topic.iloc[i_doc, new_topic] += 1\n",
    "            N_word_topic.loc[word, new_topic] += 1\n",
    "\n",
    "            Z[i_doc][i_word] = new_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is over, we are done! The last values of N_d_topic and N_word_topic will provide the estimates of the theta and beta (transposed) matrices.\n",
    "\n",
    "We can now make inference on the latent variable structure of our model, for example we can see the most recurrent words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main words wor topic 0:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n",
      "Main words wor topic 1:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n",
      "Main words wor topic 2:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n",
      "Main words wor topic 3:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n",
      "Main words wor topic 4:\n",
      "['measure', 'happen', 'More', 'planet', 'plight']\n",
      "\n",
      "Main words wor topic 5:\n",
      "['commitment', 'resolution', 'remain', 'cooperation', 'system']\n",
      "\n",
      "Main words wor topic 6:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n",
      "Main words wor topic 7:\n",
      "['imperative', 'More', 'predecessor', 'per', 'period']\n",
      "\n",
      "Main words wor topic 8:\n",
      "['political', 'economy', 'poverty', 'social', 'session']\n",
      "\n",
      "Main words wor topic 9:\n",
      "['More', 'point', 'per', 'period', 'permanent']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(f\"Main words wor topic {topic}:\")\n",
    "    print(N_word_topic.sort_values(by=topic, ascending = False).index[:5].to_list())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Code\n",
    "The code above was made for didactic purposes, sacrificing speed for understandability, since speed was never a concern in the first place. \n",
    "If we wanted, we could nonetheless speed up our code significantly with some changes. For once, we could replace the words in the corpus with their respective dictionary index. This will allow us to use numerically indexed numpy arrays in the Gibbs sampling procedure instead of slower pandas dataframes.\n",
    "\n",
    "One feature that was not implemented here is to store the values of the latent variables at each iteration of the Markov Chain. This would allow for convergence diagnostics, if memory capabilities allow for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
